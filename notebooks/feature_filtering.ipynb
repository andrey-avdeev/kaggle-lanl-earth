{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LANL Earhquake prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.options.display.float_format = '{:,.10f}'.format\n",
    "pd.options.display.max_rows = 4000\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_BASE = '../data/processed/train/features_base.csv'\n",
    "FEATURES_BASE_DENOISE = '../data/processed/train/features_base_denoise.csv'\n",
    "FEATURES_FOLDS_DENOISE = '../data/processed/train/features_folds_denoise.csv'\n",
    "FEATURES_TSFRESH = '../data/processed/train/features_tsfresh.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 107 ms, total: 1.85 s\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_base = pd.read_csv(FEATURES_BASE,index_col='id').add_prefix('b_')\n",
    "features_base_denoise = pd.read_csv(FEATURES_BASE_DENOISE,index_col='id').add_prefix('bd_')\n",
    "features_folds_denoise = pd.read_csv(FEATURES_FOLDS_DENOISE,index_col='id').add_prefix('fd_')\n",
    "features_tsfresh = pd.read_csv(FEATURES_TSFRESH,index_col='id').add_prefix('ts_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = features_base['b_y']\n",
    "df_all = features_base.join(features_base_denoise).join(features_folds_denoise).join(features_tsfresh)\n",
    "df = df_all.drop(['bd_y','fd_y','ts_y'],axis=1)\n",
    "X_all = df_all.drop(['b_y','bd_y','fd_y','ts_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_base.shape: (4194, 116)\n",
      "features_base_denoise.shape: (4194, 116)\n",
      "features_folds_denoise.shape: (4194, 1151)\n",
      "features_tsfresh.shape: (4194, 789)\n",
      "df_all.shape: (4194, 2172)\n",
      "df.shape: (4194, 2169)\n",
      "X_all.shape: (4194, 2168)\n"
     ]
    }
   ],
   "source": [
    "print('features_base.shape:',features_base.shape)\n",
    "print('features_base_denoise.shape:',features_base_denoise.shape)\n",
    "print('features_folds_denoise.shape:',features_folds_denoise.shape)\n",
    "print('features_tsfresh.shape:',features_tsfresh.shape)\n",
    "print('df_all.shape:',df_all.shape)\n",
    "print('df.shape:',df.shape)\n",
    "print('X_all.shape:',X_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning from NaN,infinity or too large values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.7 ms, sys: 411 Âµs, total: 18.1 ms\n",
      "Wall time: 16.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if np.any(np.isnan(X_all)):\n",
    "    X_all.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 18.9 ms, total: 31.1 s\n",
      "Wall time: 31.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90b71e5198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "#Using Pearson Correlation\n",
    "plt.figure(figsize=(12,10))\n",
    "cor = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation with output variable\n",
    "cor_target = abs(cor['b_y'])\n",
    "#Selecting highly correlated features\n",
    "relevant_features = cor_target[cor_target>0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['b_q90', 'b_q95', 'b_q05', 'b_q10', 'b_q01_roll_std_10',\n",
       "       'b_q05_roll_std_10', 'b_q10_roll_std_10', 'b_q95_roll_std_10',\n",
       "       'b_q05_roll_mean_10', 'b_q95_roll_mean_10', 'b_q01_roll_std_50',\n",
       "       'b_q05_roll_std_50', 'b_q10_roll_std_50', 'b_q95_roll_std_50',\n",
       "       'b_min_roll_std_100', 'b_q01_roll_std_100', 'b_q05_roll_std_100',\n",
       "       'b_q10_roll_std_100', 'b_q95_roll_std_100', 'b_min_roll_std_1000',\n",
       "       'b_q01_roll_std_1000', 'b_q05_roll_std_1000',\n",
       "       'b_q10_roll_std_1000', 'b_q95_roll_std_1000', 'b_y', 'bd_q95',\n",
       "       'bd_q05', 'bd_q95_roll_std_10', 'bd_q05_roll_mean_10',\n",
       "       'bd_q95_roll_mean_10', 'bd_q01_roll_abs_mean_10',\n",
       "       'bd_q95_roll_abs_mean_10', 'bd_q95_roll_std_50',\n",
       "       'bd_q95_roll_std_100', 'bd_q95_roll_std_1000',\n",
       "       'fd_f_0_q01_roll_abs_mean_10', 'fd_f_1_q01_roll_abs_mean_10',\n",
       "       'fd_f_2_q01_roll_abs_mean_10', 'fd_f_3_q01_roll_abs_mean_10',\n",
       "       'fd_f_4_q01_roll_abs_mean_10', 'fd_f_5_q01_roll_abs_mean_10',\n",
       "       'fd_f_6_q01_roll_abs_mean_10', 'fd_f_7_q01_roll_abs_mean_10',\n",
       "       'fd_f_8_q01_roll_abs_mean_10', 'fd_f_9_q01_roll_abs_mean_10',\n",
       "       'ts_x__agg_autocorrelation__f_agg_\"mean\"__maxlag_40',\n",
       "       'ts_x__agg_autocorrelation__f_agg_\"var\"__maxlag_40',\n",
       "       'ts_x__ar_coefficient__k_10__coeff_0',\n",
       "       'ts_x__ar_coefficient__k_10__coeff_1',\n",
       "       'ts_x__ar_coefficient__k_10__coeff_4',\n",
       "       'ts_x__augmented_dickey_fuller__attr_\"teststat\"',\n",
       "       'ts_x__autocorrelation__lag_1', 'ts_x__autocorrelation__lag_2',\n",
       "       'ts_x__autocorrelation__lag_3', 'ts_x__autocorrelation__lag_4',\n",
       "       'ts_x__autocorrelation__lag_9', 'ts_x__c3__lag_1',\n",
       "       'ts_x__c3__lag_2', 'ts_x__c3__lag_3',\n",
       "       'ts_x__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.4__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.6__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.4',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.8__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.8__ql_0.4',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.2',\n",
       "       'ts_x__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.8__ql_0.4',\n",
       "       'ts_x__cid_ce__normalize_True',\n",
       "       'ts_x__fft_aggregated__aggtype_\"centroid\"',\n",
       "       'ts_x__fft_aggregated__aggtype_\"kurtosis\"',\n",
       "       'ts_x__fft_aggregated__aggtype_\"skew\"',\n",
       "       'ts_x__fft_aggregated__aggtype_\"variance\"',\n",
       "       'ts_x__longest_strike_above_mean', 'ts_x__number_crossing_m__m_-1',\n",
       "       'ts_x__number_cwt_peaks__n_1', 'ts_x__number_cwt_peaks__n_5',\n",
       "       'ts_x__number_peaks__n_1', 'ts_x__number_peaks__n_10',\n",
       "       'ts_x__number_peaks__n_3', 'ts_x__number_peaks__n_5',\n",
       "       'ts_x__partial_autocorrelation__lag_1',\n",
       "       'ts_x__partial_autocorrelation__lag_2',\n",
       "       'ts_x__partial_autocorrelation__lag_3',\n",
       "       'ts_x__partial_autocorrelation__lag_7',\n",
       "       'ts_x__partial_autocorrelation__lag_8',\n",
       "       'ts_x__partial_autocorrelation__lag_9', 'ts_x__quantile__q_0.1',\n",
       "       'ts_x__quantile__q_0.2', 'ts_x__quantile__q_0.8',\n",
       "       'ts_x__quantile__q_0.9',\n",
       "       'ts_x__range_count__max_1000000000000.0__min_0',\n",
       "       'ts_x__ratio_beyond_r_sigma__r_0.5',\n",
       "       'ts_x__ratio_beyond_r_sigma__r_3',\n",
       "       'ts_x__ratio_beyond_r_sigma__r_5', 'ts_x__value_count__value_-1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_features.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Cross-Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from src.config.common import RANDOM_STATE\n",
    "X, X_cross, y, y_cross = train_test_split(X_all, y_all, test_size=0.15, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.grid import grid_search as gs\n",
    "from src.config.grid import GridSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ARDRegression,BayesianRidge,ElasticNet,ElasticNetCV\n",
    "from sklearn.linear_model import HuberRegressor, Lars,LarsCV,Lasso,LassoCV,LassoLars,LassoLarsCV\n",
    "from sklearn.linear_model import LassoLarsIC,LinearRegression,MultiTaskElasticNet,MultiTaskElasticNetCV\n",
    "from sklearn.linear_model import MultiTaskLasso, MultiTaskLassoCV, PassiveAggressiveRegressor, RANSACRegressor\n",
    "from sklearn.linear_model import Ridge, RidgeCV, SGDRegressor, TheilSenRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "i=0\n",
    "k_features = [50,100,200]\n",
    "scalers = []\n",
    "\n",
    "cv_dict['ridge'] = {}\n",
    "for k_feature in k_features:\n",
    "    cv_dict['ridge'][k_feature] = {\n",
    "        'best_params':[],\n",
    "        'best_score':[],\n",
    "        'time_left':[],\n",
    "        'mae':[]\n",
    "    }\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        print(k_feature)\n",
    "        i+=1\n",
    "    #     print('kfold.iteration=',i)\n",
    "        X_train, X_test, y_train, y_test = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    #     print('VarianceThreshold')\n",
    "        var_selector = VarianceThreshold().fit(X_train)\n",
    "\n",
    "    #     print('before.X.shape:',X_train.shape)\n",
    "        X_train = X_train[X_train.columns[var_selector.get_support(indices=True)]]\n",
    "    #     print('after.X.shape:',X_train.shape)\n",
    "\n",
    "    #     print('SelectKBest')\n",
    "#         pca = PCA(n_components=k_feature)\n",
    "#         pca = pca.fit(X_train)\n",
    "        \n",
    "#         X_train = pd.DataFrame(data=pca.transform(X_train))\n",
    "        \n",
    "        \n",
    "        kbest_selector = SelectKBest(f_regression, k=k_feature)\n",
    "        kbest_selector = kbest_selector.fit(X_train, y_train)\n",
    "        X_train = X_train[X_train.columns[kbest_selector.get_support(indices=True)]]\n",
    "\n",
    "    #     print('Scaler')\n",
    "#         scaler = StandardScaler()\n",
    "        scaler = RobustScaler()\n",
    "        scaler = scaler.fit(X_train)\n",
    "\n",
    "        X_train_scale = scaler.transform(X_train)\n",
    "\n",
    "        best_params, best_score, time_left = gs(Ridge(), GridSpace.ridge, X_train_scale,y_train)\n",
    "        print(best_params,'loss:',best_score,'minutes_left:',time_left,'k_features_count:',k_feature)\n",
    "\n",
    "    #     test dataset\n",
    "        \n",
    "        X_test = X_test[X_test.columns[var_selector.get_support(indices=True)]]\n",
    "#         X_test = pca.transform(X_test)\n",
    "        X_test = X_test[X_test.columns[kbest_selector.get_support(indices=True)]]\n",
    "        X_test_scale = scaler.transform(X_test)\n",
    "\n",
    "        model = Ridge(**best_params)\n",
    "        model.fit(X_train_scale,y_train)\n",
    "        prediction = model.predict(X_test_scale)\n",
    "\n",
    "        cv_dict['ridge'][k_feature]['best_params'].append(best_params)\n",
    "        cv_dict['ridge'][k_feature]['best_score'].append(best_score)\n",
    "        cv_dict['ridge'][k_feature]['time_left'].append(time_left)\n",
    "        cv_dict['ridge'][k_feature]['mae'].append(mean_absolute_error(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cv(cv_dict):\n",
    "    for model_name,cv in cv_dict.items():\n",
    "        for key,val in cv.items():\n",
    "            print(model_name,key,np.mean(val['mae']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv(cv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cv(cv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 5\n",
    "\n",
    "def make_predictions(estimator, features, target, test=None, plot=True, lgb=False):\n",
    "    \"\"\"Train the estimator and make predictions for oof and test data.\"\"\"\n",
    "    folds = KFold(num_folds, shuffle=True, random_state=2019)\n",
    "    oof_predictions = np.zeros(features.shape[0])\n",
    "    if test is not None:\n",
    "        sub_predictions = np.zeros(test.shape[0])\n",
    "    for (train_index, valid_index) in folds.split(features, target):\n",
    "        \n",
    "        if lgb:\n",
    "            estimator.fit(features[train_index], target[train_index],\n",
    "                          early_stopping_rounds=100, verbose=False,\n",
    "                          eval_set=[(features[train_index], target[train_index]),\n",
    "                                    (features[valid_index], target[valid_index])])\n",
    "        else:\n",
    "            estimator.fit(features[train_index], target[train_index])\n",
    "        oof_predictions[valid_index] = estimator.predict(features[valid_index]).flatten()\n",
    "        if test is not None:\n",
    "            sub_predictions += estimator.predict(test).flatten() / num_folds\n",
    "    \n",
    "    # Plot out-of-fold predictions vs actual values\n",
    "    if plot:\n",
    "        fig, axis = plt.subplots(1, 2, figsize=(12,5))\n",
    "        ax1, ax2 = axis\n",
    "        ax1.set_xlabel('actual')\n",
    "        ax1.set_ylabel('predicted')\n",
    "        ax2.set_xlabel('train index')\n",
    "        ax2.set_ylabel('time to failure')\n",
    "        ax1.scatter(target, oof_predictions, color='brown')\n",
    "        ax1.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)], color='blue')\n",
    "        ax2.plot(target, color='blue', label='y_train')\n",
    "        ax2.plot(oof_predictions, color='orange')\n",
    "    if test is not None:\n",
    "        return oof_predictions, sub_predictions\n",
    "    else:\n",
    "        return oof_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "    'objective': 'regression_l1',\n",
    "    'boosting': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'random_seed': 19,\n",
    "    'n_estimators': 20000,\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16, -1],\n",
    "    'feature_fraction': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'subsample': [0.8, 0.85, 0.9, 0.95, 1],\n",
    "    'lambda_l1': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'lambda_l2': [0, 0.1, 0.2, 0.4, 0.6, 0.9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'min_gain_to_split': [0, 0.001, 0.01, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_selector = VarianceThreshold().fit(X)\n",
    "\n",
    "#     print('before.X.shape:',X_train.shape)\n",
    "X_train = X_train[X_train.columns[var_selector.get_support(indices=True)]]\n",
    "#     print('after.X.shape:',X_train.shape)\n",
    "\n",
    "#     print('SelectKBest')\n",
    "#         pca = PCA(n_components=k_feature)\n",
    "#         pca = pca.fit(X_train)\n",
    "\n",
    "#         X_train = pd.DataFrame(data=pca.transform(X_train))\n",
    "\n",
    "\n",
    "kbest_selector = SelectKBest(f_regression, k=k_feature)\n",
    "kbest_selector = kbest_selector.fit(X_train, y_train)\n",
    "X_train = X_train[X_train.columns[kbest_selector.get_support(indices=True)]]\n",
    "\n",
    "#     print('Scaler')\n",
    "#         scaler = StandardScaler()\n",
    "scaler = RobustScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_scale = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import lightgbm as lgb\n",
    "dataset = lgb.Dataset(X, label=y)  # no need to scale features\n",
    "\n",
    "best_score = 999\n",
    "best_params= None\n",
    "best_nrounds = None\n",
    "\n",
    "for i in range(500):\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    params.update(fixed_params)\n",
    "    result = lgb.cv(params, dataset, nfold=5, early_stopping_rounds=100,\n",
    "                    stratified=False)\n",
    "    \n",
    "    if result['l1-mean'][-1] < best_score:\n",
    "        best_score = result['l1-mean'][-1]\n",
    "        best_params = params\n",
    "        best_nrounds = len(result['l1-mean'])\n",
    "        \n",
    "    print(i,best_score,best_nrounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best mean score: {:.4f}, num rounds: {}\".format(best_score, best_nrounds))\n",
    "print(best_params)\n",
    "gb_oof = make_predictions(lgb.LGBMRegressor(**best_params), X.values, y, lgb=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
